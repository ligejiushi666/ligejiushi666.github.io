卷积神经网络
-----
1.卷积神经网络基本原理
>【稀疏交互(sparse interactions)】在卷积神经网络中，卷积核的尺度是远小于输入的维度，这样每个输出神经元只与前一层局部区域存在连接权重。其物理意义是，其实我们现实世界中的数据都是具有局部特征的，我们可以先学习到局部特征，再将其组合成抽象复杂的特征。\
>【参数共享】不同的函数共享同一参数。在CNN中即，每个位置提取特征的时候是共享同一卷积核。\
>【平移不变性】卷积函数具有平移不变性(ps. 池化也具有一定的平移不变性)。\
>【卷积操作】卷积核翻转相乘加和。同时单个卷积核只能够提取到一个特征。\
>【池化】常见的池化操作分为最大池化和均值池化，池化的本质就是降采样。其作用有①降维；②保持平移不变性（图像若有小范围平移，对池化的影响不大，因为池化是对整个邻居范围的统计）；③定长输出， 无论特征图有多大，通过池化后都会得到一个scaler，scaler再拼接成一个定长的向量。\
2.卷积神经网络的一些特点
>【1X1卷积的作用】1.升维降维(in_channel -> out_channel) 2.非线性 与全连接层的区别：输入尺寸是否可变，全连接层的输入尺寸是固定的，卷积层的输入尺寸是任意的。\
>【一些小问题】卷积核的通道数必须和输入尺寸的通道数相等。输出的通道数为卷积核的个数。\

循环神经网络
----

Transformer
----

